{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac9fddc8",
   "metadata": {},
   "source": [
    "# **Getting Started With Spark using Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd7c56",
   "metadata": {},
   "source": [
    "![Spark Logo](http://spark.apache.org/images/spark-logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47096bf2",
   "metadata": {},
   "source": [
    "## How to run Jupyter notebook cell ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00d149b",
   "metadata": {},
   "source": [
    "Jupyter Notebooks are a powerful way to write and iterate on your Python code for data analysis.The Jupyter Notebook is organized into cells. Each cell can contain code, text, or visual elements. \n",
    "Rather than writing and re-writing an entire program, Jupyter Notebooks allow you to write code in separate blocks (or “cells”) and run each block of code individually. \n",
    "Then, if you need to make a change, you can go back and make an edit and rerun the program again, all in the same window.\n",
    "\n",
    "To select the cell, simply click on it. The selected cell will be highlighted, indicating that it is ready for execution.Once you have the desired cell selected, you have multiple options to run it: \n",
    "* Keyboard Shortcut: Press Shift + Enter on your keyboard to run the selected cell. This will execute the code or process the content within the cell and display the output below it. \n",
    "If the cell has any output, it will be shown just after running.\n",
    "* Toolbar: Look for the \"Run Cell\" button in the toolbar at the top of the Theia Lab interface. Clicking on this button will also execute the selected cell.\n",
    "After running the cell, you can see the output displayed below it. If the cell contains any code that produces visual outputs, such as plots or images, they will be shown in the output area.\n",
    "\n",
    "To run subsequent cells, repeat the same steps: select the next cell you want to execute and use the appropriate method mentioned above.\n",
    "Remember to execute cells in the correct order if there are dependencies between them.\n",
    "For example,if you have variables defined in a previous cell that are needed in the current cell, make sure to run the preceding cell first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f77e641",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb0ecab",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31955955",
   "metadata": {},
   "source": [
    "In this lab, we will go over the basics of Apache Spark and PySpark. We will start with creating the SparkContext and SparkSession. We then create a dataframe and demonstrate the basics dataframes and SparkSQL. Finally we create an RDD and apply some basic transformations and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be24263",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cfe11509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.9/dist-packages (3.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (3.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (6.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib) (3.19.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.9/dist-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.9/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: findspark in /usr/local/lib/python3.9/dist-packages (2.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pyarrow in /usr/local/lib/python3.9/dist-packages (16.1.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from pyarrow) (1.26.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install matplotlib\n",
    "!pip install pandas\n",
    "!pip install findspark\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f1e3b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa34be5",
   "metadata": {},
   "source": [
    "## Spark Context and Spark Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad8b763",
   "metadata": {},
   "source": [
    "You will create the Spark Context and initialize the Spark session needed for SparkSQL and DataFrames.\n",
    "SparkContext is the entry point for Spark applications and contains functions to create RDDs such as `parallelize()`. SparkSession is needed for SparkSQL and DataFrame operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ec36f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a spark context class\n",
    "sc = SparkContext()\n",
    "\n",
    "# # Creating a spark session\n",
    "spark = SparkSession.builder.appName(\"pyspark-notebook\").master(\"spark://spark-master:7077\").config(\"spark.executor.memory\", \"1024m\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "001e4be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ba5954ddaaaa:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd20d19d370>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b045987b",
   "metadata": {},
   "source": [
    "#### Initialize Spark session\n",
    "To work with dataframes we just need to verify that the spark session instance has been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a65d5965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession is active and ready to use.\n"
     ]
    }
   ],
   "source": [
    "if 'spark' in locals() and isinstance(spark, SparkSession):\n",
    "    print(\"SparkSession is active and ready to use.\")\n",
    "else:\n",
    "    print(\"SparkSession is not active. Please create a SparkSession.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75fa1f9",
   "metadata": {},
   "source": [
    "## Load and Display dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a88a348",
   "metadata": {},
   "source": [
    "In this section, you will first read the CSV file into a Pandas DataFrame and then read it into a Spark DataFrame.\n",
    "Pandas is a library used for data manipulation and analysis. Pandas offers data structures and operations for creating and manipulating Data Series and DataFrame objects. Data can be imported from various data sources, e.g., Numpy arrays, Python dictionaries, and CSV files. Pandas allows you to manipulate, organize and display the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5132fb",
   "metadata": {},
   "source": [
    "| colIndex | colName            | units/description                                                                             |\n",
    "|----------|--------------------|-----------------------------------------------------------------------------------------------|\n",
    "| [1]    | work_year          | The year in which the data was collected (2024).                                              |\n",
    "| [2]    | experience_level   | The experience level of the employee, categorized as SE (Senior Engineer), MI (Mid-Level Engineer), or EL (Entry-Level Engineer). |\n",
    "| [3]    | employment_type    | The type of employment, such as full-time (FT), part-time (PT), contract (C), or freelance (F).|\n",
    "| [4]    | job_title          | The title or role of the employee within the company, for example, AI Engineer.               |\n",
    "| [5]    | salary             | The salary of the employee in the local currency (e.g., 202,730 USD).                         |\n",
    "| [6]    | salary_currency    | The currency in which the salary is denominated (e.g., USD).                                  |\n",
    "| [7]    | salary_in_usd      | The salary converted to US dollars for standardization purposes (e.g., 202,730 USD).          |\n",
    "| [8]    | employee_residence | The country of residence of the employee.                                                     |\n",
    "| [9]    | remote_ratio       | The ratio indicating the extent of remote work allowed in the position (0 for no remote work, 100 for fully remote). |\n",
    "| [10]    | company_location   | The location of the company where the employee is employed.                                   |\n",
    "| [11]    | company_size       | The size of the company, often categorized by the number of employees (S for small, M for medium, L for large). |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3901f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file using `read_csv` function in pandas\n",
    "file_path = './data/salaries.csv'\n",
    "\n",
    "data_engineer_salary = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "91f71151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c2454 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_c2454_row0_col0, #T_c2454_row0_col1, #T_c2454_row0_col2, #T_c2454_row0_col3, #T_c2454_row0_col4, #T_c2454_row0_col5, #T_c2454_row0_col6, #T_c2454_row0_col7, #T_c2454_row0_col8, #T_c2454_row0_col9, #T_c2454_row0_col10, #T_c2454_row1_col0, #T_c2454_row1_col1, #T_c2454_row1_col2, #T_c2454_row1_col3, #T_c2454_row1_col4, #T_c2454_row1_col5, #T_c2454_row1_col6, #T_c2454_row1_col7, #T_c2454_row1_col8, #T_c2454_row1_col9, #T_c2454_row1_col10, #T_c2454_row2_col0, #T_c2454_row2_col1, #T_c2454_row2_col2, #T_c2454_row2_col3, #T_c2454_row2_col4, #T_c2454_row2_col5, #T_c2454_row2_col6, #T_c2454_row2_col7, #T_c2454_row2_col8, #T_c2454_row2_col9, #T_c2454_row2_col10, #T_c2454_row3_col0, #T_c2454_row3_col1, #T_c2454_row3_col2, #T_c2454_row3_col3, #T_c2454_row3_col4, #T_c2454_row3_col5, #T_c2454_row3_col6, #T_c2454_row3_col7, #T_c2454_row3_col8, #T_c2454_row3_col9, #T_c2454_row3_col10, #T_c2454_row4_col0, #T_c2454_row4_col1, #T_c2454_row4_col2, #T_c2454_row4_col3, #T_c2454_row4_col4, #T_c2454_row4_col5, #T_c2454_row4_col6, #T_c2454_row4_col7, #T_c2454_row4_col8, #T_c2454_row4_col9, #T_c2454_row4_col10, #T_c2454_row5_col0, #T_c2454_row5_col1, #T_c2454_row5_col2, #T_c2454_row5_col3, #T_c2454_row5_col4, #T_c2454_row5_col5, #T_c2454_row5_col6, #T_c2454_row5_col7, #T_c2454_row5_col8, #T_c2454_row5_col9, #T_c2454_row5_col10, #T_c2454_row6_col0, #T_c2454_row6_col1, #T_c2454_row6_col2, #T_c2454_row6_col3, #T_c2454_row6_col4, #T_c2454_row6_col5, #T_c2454_row6_col6, #T_c2454_row6_col7, #T_c2454_row6_col8, #T_c2454_row6_col9, #T_c2454_row6_col10, #T_c2454_row7_col0, #T_c2454_row7_col1, #T_c2454_row7_col2, #T_c2454_row7_col3, #T_c2454_row7_col4, #T_c2454_row7_col5, #T_c2454_row7_col6, #T_c2454_row7_col7, #T_c2454_row7_col8, #T_c2454_row7_col9, #T_c2454_row7_col10, #T_c2454_row8_col0, #T_c2454_row8_col1, #T_c2454_row8_col2, #T_c2454_row8_col3, #T_c2454_row8_col4, #T_c2454_row8_col5, #T_c2454_row8_col6, #T_c2454_row8_col7, #T_c2454_row8_col8, #T_c2454_row8_col9, #T_c2454_row8_col10, #T_c2454_row9_col0, #T_c2454_row9_col1, #T_c2454_row9_col2, #T_c2454_row9_col3, #T_c2454_row9_col4, #T_c2454_row9_col5, #T_c2454_row9_col6, #T_c2454_row9_col7, #T_c2454_row9_col8, #T_c2454_row9_col9, #T_c2454_row9_col10 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c2454\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c2454_level0_col0\" class=\"col_heading level0 col0\" >work_year</th>\n",
       "      <th id=\"T_c2454_level0_col1\" class=\"col_heading level0 col1\" >experience_level</th>\n",
       "      <th id=\"T_c2454_level0_col2\" class=\"col_heading level0 col2\" >employment_type</th>\n",
       "      <th id=\"T_c2454_level0_col3\" class=\"col_heading level0 col3\" >job_title</th>\n",
       "      <th id=\"T_c2454_level0_col4\" class=\"col_heading level0 col4\" >salary</th>\n",
       "      <th id=\"T_c2454_level0_col5\" class=\"col_heading level0 col5\" >salary_currency</th>\n",
       "      <th id=\"T_c2454_level0_col6\" class=\"col_heading level0 col6\" >salary_in_usd</th>\n",
       "      <th id=\"T_c2454_level0_col7\" class=\"col_heading level0 col7\" >employee_residence</th>\n",
       "      <th id=\"T_c2454_level0_col8\" class=\"col_heading level0 col8\" >remote_ratio</th>\n",
       "      <th id=\"T_c2454_level0_col9\" class=\"col_heading level0 col9\" >company_location</th>\n",
       "      <th id=\"T_c2454_level0_col10\" class=\"col_heading level0 col10\" >company_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c2454_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_c2454_row0_col0\" class=\"data row0 col0\" >2024</td>\n",
       "      <td id=\"T_c2454_row0_col1\" class=\"data row0 col1\" >SE</td>\n",
       "      <td id=\"T_c2454_row0_col2\" class=\"data row0 col2\" >FT</td>\n",
       "      <td id=\"T_c2454_row0_col3\" class=\"data row0 col3\" >AI Engineer</td>\n",
       "      <td id=\"T_c2454_row0_col4\" class=\"data row0 col4\" >202730</td>\n",
       "      <td id=\"T_c2454_row0_col5\" class=\"data row0 col5\" >USD</td>\n",
       "      <td id=\"T_c2454_row0_col6\" class=\"data row0 col6\" >202730</td>\n",
       "      <td id=\"T_c2454_row0_col7\" class=\"data row0 col7\" >US</td>\n",
       "      <td id=\"T_c2454_row0_col8\" class=\"data row0 col8\" >0</td>\n",
       "      <td id=\"T_c2454_row0_col9\" class=\"data row0 col9\" >US</td>\n",
       "      <td id=\"T_c2454_row0_col10\" class=\"data row0 col10\" >M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2454_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_c2454_row1_col0\" class=\"data row1 col0\" >2024</td>\n",
       "      <td id=\"T_c2454_row1_col1\" class=\"data row1 col1\" >SE</td>\n",
       "      <td id=\"T_c2454_row1_col2\" class=\"data row1 col2\" >FT</td>\n",
       "      <td id=\"T_c2454_row1_col3\" class=\"data row1 col3\" >AI Engineer</td>\n",
       "      <td id=\"T_c2454_row1_col4\" class=\"data row1 col4\" >92118</td>\n",
       "      <td id=\"T_c2454_row1_col5\" class=\"data row1 col5\" >USD</td>\n",
       "      <td id=\"T_c2454_row1_col6\" class=\"data row1 col6\" >92118</td>\n",
       "      <td id=\"T_c2454_row1_col7\" class=\"data row1 col7\" >US</td>\n",
       "      <td id=\"T_c2454_row1_col8\" class=\"data row1 col8\" >0</td>\n",
       "      <td id=\"T_c2454_row1_col9\" class=\"data row1 col9\" >US</td>\n",
       "      <td id=\"T_c2454_row1_col10\" class=\"data row1 col10\" >M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2454_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_c2454_row2_col0\" class=\"data row2 col0\" >2024</td>\n",
       "      <td id=\"T_c2454_row2_col1\" class=\"data row2 col1\" >SE</td>\n",
       "      <td id=\"T_c2454_row2_col2\" class=\"data row2 col2\" >FT</td>\n",
       "      <td id=\"T_c2454_row2_col3\" class=\"data row2 col3\" >Data Engineer</td>\n",
       "      <td id=\"T_c2454_row2_col4\" class=\"data row2 col4\" >130500</td>\n",
       "      <td id=\"T_c2454_row2_col5\" class=\"data row2 col5\" >USD</td>\n",
       "      <td id=\"T_c2454_row2_col6\" class=\"data row2 col6\" >130500</td>\n",
       "      <td id=\"T_c2454_row2_col7\" class=\"data row2 col7\" >US</td>\n",
       "      <td id=\"T_c2454_row2_col8\" class=\"data row2 col8\" >0</td>\n",
       "      <td id=\"T_c2454_row2_col9\" class=\"data row2 col9\" >US</td>\n",
       "      <td id=\"T_c2454_row2_col10\" class=\"data row2 col10\" >M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2454_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_c2454_row3_col0\" class=\"data row3 col0\" >2024</td>\n",
       "      <td id=\"T_c2454_row3_col1\" class=\"data row3 col1\" >SE</td>\n",
       "      <td id=\"T_c2454_row3_col2\" class=\"data row3 col2\" >FT</td>\n",
       "      <td id=\"T_c2454_row3_col3\" class=\"data row3 col3\" >Data Engineer</td>\n",
       "      <td id=\"T_c2454_row3_col4\" class=\"data row3 col4\" >96000</td>\n",
       "      <td id=\"T_c2454_row3_col5\" class=\"data row3 col5\" >USD</td>\n",
       "      <td id=\"T_c2454_row3_col6\" class=\"data row3 col6\" >96000</td>\n",
       "      <td id=\"T_c2454_row3_col7\" class=\"data row3 col7\" >US</td>\n",
       "      <td id=\"T_c2454_row3_col8\" class=\"data row3 col8\" >0</td>\n",
       "      <td id=\"T_c2454_row3_col9\" class=\"data row3 col9\" >US</td>\n",
       "      <td id=\"T_c2454_row3_col10\" class=\"data row3 col10\" >M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2454_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_c2454_row4_col0\" class=\"data row4 col0\" >2024</td>\n",
       "      <td id=\"T_c2454_row4_col1\" class=\"data row4 col1\" >SE</td>\n",
       "      <td id=\"T_c2454_row4_col2\" class=\"data row4 col2\" >FT</td>\n",
       "      <td id=\"T_c2454_row4_col3\" class=\"data row4 col3\" >Machine Learning Engineer</td>\n",
       "      <td id=\"T_c2454_row4_col4\" class=\"data row4 col4\" >190000</td>\n",
       "      <td id=\"T_c2454_row4_col5\" class=\"data row4 col5\" >USD</td>\n",
       "      <td id=\"T_c2454_row4_col6\" class=\"data row4 col6\" >190000</td>\n",
       "      <td id=\"T_c2454_row4_col7\" class=\"data row4 col7\" >US</td>\n",
       "      <td id=\"T_c2454_row4_col8\" class=\"data row4 col8\" >0</td>\n",
       "      <td id=\"T_c2454_row4_col9\" class=\"data row4 col9\" >US</td>\n",
       "      <td id=\"T_c2454_row4_col10\" class=\"data row4 col10\" >M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2454_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_c2454_row5_col0\" class=\"data row5 col0\" >2024</td>\n",
       "      <td id=\"T_c2454_row5_col1\" class=\"data row5 col1\" >SE</td>\n",
       "      <td id=\"T_c2454_row5_col2\" class=\"data row5 col2\" >FT</td>\n",
       "      <td id=\"T_c2454_row5_col3\" class=\"data row5 col3\" >Machine Learning Engineer</td>\n",
       "      <td id=\"T_c2454_row5_col4\" class=\"data row5 col4\" >160000</td>\n",
       "      <td id=\"T_c2454_row5_col5\" class=\"data row5 col5\" >USD</td>\n",
       "      <td id=\"T_c2454_row5_col6\" class=\"data row5 col6\" >160000</td>\n",
       "      <td id=\"T_c2454_row5_col7\" class=\"data row5 col7\" >US</td>\n",
       "      <td id=\"T_c2454_row5_col8\" class=\"data row5 col8\" >0</td>\n",
       "      <td id=\"T_c2454_row5_col9\" class=\"data row5 col9\" >US</td>\n",
       "      <td id=\"T_c2454_row5_col10\" class=\"data row5 col10\" >M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2454_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_c2454_row6_col0\" class=\"data row6 col0\" >2024</td>\n",
       "      <td id=\"T_c2454_row6_col1\" class=\"data row6 col1\" >MI</td>\n",
       "      <td id=\"T_c2454_row6_col2\" class=\"data row6 col2\" >FT</td>\n",
       "      <td id=\"T_c2454_row6_col3\" class=\"data row6 col3\" >ML Engineer</td>\n",
       "      <td id=\"T_c2454_row6_col4\" class=\"data row6 col4\" >400000</td>\n",
       "      <td id=\"T_c2454_row6_col5\" class=\"data row6 col5\" >USD</td>\n",
       "      <td id=\"T_c2454_row6_col6\" class=\"data row6 col6\" >400000</td>\n",
       "      <td id=\"T_c2454_row6_col7\" class=\"data row6 col7\" >US</td>\n",
       "      <td id=\"T_c2454_row6_col8\" class=\"data row6 col8\" >0</td>\n",
       "      <td id=\"T_c2454_row6_col9\" class=\"data row6 col9\" >US</td>\n",
       "      <td id=\"T_c2454_row6_col10\" class=\"data row6 col10\" >M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2454_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_c2454_row7_col0\" class=\"data row7 col0\" >2024</td>\n",
       "      <td id=\"T_c2454_row7_col1\" class=\"data row7 col1\" >MI</td>\n",
       "      <td id=\"T_c2454_row7_col2\" class=\"data row7 col2\" >FT</td>\n",
       "      <td id=\"T_c2454_row7_col3\" class=\"data row7 col3\" >ML Engineer</td>\n",
       "      <td id=\"T_c2454_row7_col4\" class=\"data row7 col4\" >65000</td>\n",
       "      <td id=\"T_c2454_row7_col5\" class=\"data row7 col5\" >USD</td>\n",
       "      <td id=\"T_c2454_row7_col6\" class=\"data row7 col6\" >65000</td>\n",
       "      <td id=\"T_c2454_row7_col7\" class=\"data row7 col7\" >US</td>\n",
       "      <td id=\"T_c2454_row7_col8\" class=\"data row7 col8\" >0</td>\n",
       "      <td id=\"T_c2454_row7_col9\" class=\"data row7 col9\" >US</td>\n",
       "      <td id=\"T_c2454_row7_col10\" class=\"data row7 col10\" >M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2454_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_c2454_row8_col0\" class=\"data row8 col0\" >2024</td>\n",
       "      <td id=\"T_c2454_row8_col1\" class=\"data row8 col1\" >EN</td>\n",
       "      <td id=\"T_c2454_row8_col2\" class=\"data row8 col2\" >FT</td>\n",
       "      <td id=\"T_c2454_row8_col3\" class=\"data row8 col3\" >Data Analyst</td>\n",
       "      <td id=\"T_c2454_row8_col4\" class=\"data row8 col4\" >101520</td>\n",
       "      <td id=\"T_c2454_row8_col5\" class=\"data row8 col5\" >USD</td>\n",
       "      <td id=\"T_c2454_row8_col6\" class=\"data row8 col6\" >101520</td>\n",
       "      <td id=\"T_c2454_row8_col7\" class=\"data row8 col7\" >US</td>\n",
       "      <td id=\"T_c2454_row8_col8\" class=\"data row8 col8\" >0</td>\n",
       "      <td id=\"T_c2454_row8_col9\" class=\"data row8 col9\" >US</td>\n",
       "      <td id=\"T_c2454_row8_col10\" class=\"data row8 col10\" >M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2454_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_c2454_row9_col0\" class=\"data row9 col0\" >2024</td>\n",
       "      <td id=\"T_c2454_row9_col1\" class=\"data row9 col1\" >EN</td>\n",
       "      <td id=\"T_c2454_row9_col2\" class=\"data row9 col2\" >FT</td>\n",
       "      <td id=\"T_c2454_row9_col3\" class=\"data row9 col3\" >Data Analyst</td>\n",
       "      <td id=\"T_c2454_row9_col4\" class=\"data row9 col4\" >45864</td>\n",
       "      <td id=\"T_c2454_row9_col5\" class=\"data row9 col5\" >USD</td>\n",
       "      <td id=\"T_c2454_row9_col6\" class=\"data row9 col6\" >45864</td>\n",
       "      <td id=\"T_c2454_row9_col7\" class=\"data row9 col7\" >US</td>\n",
       "      <td id=\"T_c2454_row9_col8\" class=\"data row9 col8\" >0</td>\n",
       "      <td id=\"T_c2454_row9_col9\" class=\"data row9 col9\" >US</td>\n",
       "      <td id=\"T_c2454_row9_col10\" class=\"data row9 col10\" >M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fd20d1df9a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display only the first 10 rows with styles\n",
    "styled_salaries = data_engineer_salary.head(10).style.set_properties(**{'text-align': 'left'}) \n",
    "styled_salaries.set_table_styles([{'selector': 'th', 'props': [('text-align', 'left')]}])\n",
    "\n",
    "# Display the styled DataFrame\n",
    "display(styled_salaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd92e242",
   "metadata": {},
   "source": [
    "## DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443262d9",
   "metadata": {},
   "source": [
    "A DataFrame is two-dimensional. Columns can be of different data types. DataFrames accept many data inputs including series and other DataFrames. You can pass indexes (row labels) and columns (column labels). Indexes can be numbers, dates, or strings/tuples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45dd33e",
   "metadata": {},
   "source": [
    "### Load the data into Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f1665a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"work_year\", IntegerType(), True),\n",
    "    StructField(\"experience_level\", StringType(), True),\n",
    "    StructField(\"employment_type\", StringType(), True),\n",
    "    StructField(\"job_title\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"salary_currency\", StringType(), True),\n",
    "    StructField(\"salary_in_usd\", IntegerType(), True),\n",
    "    StructField(\"employee_residence\", StringType(), True),\n",
    "    StructField(\"remote_ratio\", IntegerType(), True),\n",
    "    StructField(\"company_location\", StringType(), True),\n",
    "    StructField(\"company_size\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f51e069c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- work_year: integer (nullable = true)\n",
      " |-- experience_level: string (nullable = true)\n",
      " |-- employment_type: string (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- salary_currency: string (nullable = true)\n",
      " |-- salary_in_usd: integer (nullable = true)\n",
      " |-- employee_residence: string (nullable = true)\n",
      " |-- remote_ratio: integer (nullable = true)\n",
      " |-- company_location: string (nullable = true)\n",
      " |-- company_size: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert Pandas DataFrame to Spark DataFrame with defined schema\n",
    "salaries_df = spark.createDataFrame(data_engineer_salary.values.tolist(), schema=schema)\n",
    "# Show the DataFrame schema and some data\n",
    "salaries_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94602f8d",
   "metadata": {},
   "source": [
    "###  Basic data analysis and manipulation\n",
    "In this section, we perform basic data analysis and manipulation. We start with previewing the data and then applying some filtering and columwise operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc66a00b",
   "metadata": {},
   "source": [
    "#### Task 1:\n",
    "\n",
    "Show the first 5 records of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "05dc1875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "|work_year|experience_level|employment_type|           job_title|salary|salary_currency|salary_in_usd|employee_residence|remote_ratio|company_location|company_size|\n",
      "+---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "|     2024|              SE|             FT|         AI Engineer|202730|            USD|       202730|                US|           0|              US|           M|\n",
      "|     2024|              SE|             FT|         AI Engineer| 92118|            USD|        92118|                US|           0|              US|           M|\n",
      "|     2024|              SE|             FT|       Data Engineer|130500|            USD|       130500|                US|           0|              US|           M|\n",
      "|     2024|              SE|             FT|       Data Engineer| 96000|            USD|        96000|                US|           0|              US|           M|\n",
      "|     2024|              SE|             FT|Machine Learning ...|190000|            USD|       190000|                US|           0|              US|           M|\n",
      "+---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# to-do\n",
    "salaries_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c24a9",
   "metadata": {},
   "source": [
    "#### Task 2:\n",
    "Select the `salary` and `job_title` columns from the DataFrame and display the first 5 rows of this columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "54529a9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|salary|           job_title|\n",
      "+------+--------------------+\n",
      "|202730|         AI Engineer|\n",
      "| 92118|         AI Engineer|\n",
      "|130500|       Data Engineer|\n",
      "| 96000|       Data Engineer|\n",
      "|190000|Machine Learning ...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to-do\n",
    "salaries_df.select('salary', 'job_title').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd1a19a",
   "metadata": {},
   "source": [
    "#### Task 3: \n",
    "\n",
    "Display the first five rows of `salary` and `job_title` and `salary_currency` from the DataFrame where the salary is less than 50,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "500307d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+---------------+\n",
      "|salary|      job_title|salary_currency|\n",
      "+------+---------------+---------------+\n",
      "| 45864|   Data Analyst|            USD|\n",
      "| 45000| Data Architect|            GBP|\n",
      "| 30000|   Data Analyst|            EUR|\n",
      "| 25800|   Data Analyst|            EUR|\n",
      "| 45607|Data Specialist|            GBP|\n",
      "+------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to-do\n",
    "salaries_df.select('salary', 'job_title', 'salary_currency').filter(col('salary') < 50000).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccf8764",
   "metadata": {},
   "source": [
    "#### Task 4: \n",
    "\n",
    "Multiply the salary column by 0.5 and display the `salary`, `job_title`, and the newly calculated half_salary for the first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fa2b64d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|  salary|           job_title|\n",
      "+--------+--------------------+\n",
      "|101365.0|         AI Engineer|\n",
      "| 46059.0|         AI Engineer|\n",
      "| 65250.0|       Data Engineer|\n",
      "| 48000.0|       Data Engineer|\n",
      "| 95000.0|Machine Learning ...|\n",
      "+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to-do\n",
    "salaries_df.withColumn('salary', col('salary') * 0.5).select('salary', 'job_title').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaf14ad",
   "metadata": {},
   "source": [
    "#### Task 5: \n",
    "\n",
    "Join these two dataframes on `emp_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1c4d7b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame 1 \n",
    "\n",
    "data = [(\"A101\", \"John\"), (\"A102\", \"Peter\"), (\"A103\", \"Charlie\")] \n",
    "\n",
    "columns = [\"emp_id\", \"emp_name\"] \n",
    "\n",
    "dataframe_1 = spark.createDataFrame(data, columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "958014e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame 2 \n",
    "\n",
    "data = [(\"A101\", 1000), (\"A102\", 2000), (\"A103\", 3000)]\n",
    "\n",
    "columns = [\"emp_id\", \"salary\"]\n",
    "\n",
    "dataframe_2 = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1ba28eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+------+\n",
      "|emp_id|emp_name|emp_id|salary|\n",
      "+------+--------+------+------+\n",
      "|  A103| Charlie|  A103|  3000|\n",
      "|  A102|   Peter|  A102|  2000|\n",
      "|  A101|    John|  A101|  1000|\n",
      "+------+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to-do \n",
    "# create a new DataFrame, \"combined_df\" by performing an inner join\n",
    "combined_df = dataframe_1.join(dataframe_2, dataframe_1.emp_id == dataframe_2.emp_id, 'inner')\n",
    "combined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67240448",
   "metadata": {},
   "source": [
    "#### Task6:\n",
    "\n",
    "Filter the DataFrame to include only entries with experience levels \"SE\" (Senior Engineer) and \"MI\" (Mid-Level Engineer), then calculate and display the average salary for each experience level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1fb6e5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n",
      "|experience_level|    average_salary|\n",
      "+----------------+------------------+\n",
      "|              MI|157116.97672114908|\n",
      "|              SE|168852.49343955013|\n",
      "+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to-do \n",
    "filtered_df = salaries_df.filter((col('experience_level') == 'SE') | (col('experience_level') == 'MI'))\n",
    "filtered_df.groupBy(col('experience_level')).agg(avg('salary').alias('average_salary')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaefcc27",
   "metadata": {},
   "source": [
    "#### Task7\n",
    "Write datadrame in the `HDFS` and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "becbf1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "|work_year|experience_level|employment_type|           job_title|salary|salary_currency|salary_in_usd|employee_residence|remote_ratio|company_location|company_size|\n",
      "+---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "|     2023|              MI|             FT|       Data Engineer|175000|            USD|       175000|                US|           0|              US|           M|\n",
      "|     2023|              SE|             FT|      Data Scientist|136000|            USD|       136000|                US|         100|              US|           M|\n",
      "|     2023|              SE|             FT|      Data Scientist|104000|            USD|       104000|                US|         100|              US|           M|\n",
      "|     2023|              MI|             FT|        Data Analyst| 80000|            USD|        80000|                US|           0|              US|           M|\n",
      "|     2023|              MI|             FT|        Data Analyst| 52500|            USD|        52500|                US|           0|              US|           M|\n",
      "|     2023|              SE|             FT|      Data Scientist|110000|            USD|       110000|                US|         100|              US|           M|\n",
      "|     2023|              SE|             FT|      Data Scientist| 84000|            USD|        84000|                US|         100|              US|           M|\n",
      "|     2023|              SE|             FT|          BI Analyst|125000|            USD|       125000|                US|           0|              US|           M|\n",
      "|     2023|              SE|             FT|          BI Analyst|110000|            USD|       110000|                US|           0|              US|           M|\n",
      "|     2023|              MI|             FT|        Data Analyst| 90000|            USD|        90000|                US|           0|              US|           M|\n",
      "|     2023|              MI|             FT|        Data Analyst| 80000|            USD|        80000|                US|           0|              US|           M|\n",
      "|     2023|              SE|             FT|         ML Engineer|200000|            USD|       200000|                US|           0|              US|           M|\n",
      "|     2023|              SE|             FT|         ML Engineer|135000|            USD|       135000|                US|           0|              US|           M|\n",
      "|     2022|              EN|             FT|Business Data Ana...| 48000|            USD|        48000|                US|          50|              US|           L|\n",
      "|     2023|              EN|             FT|        AI Developer|120000|            USD|       120000|                BA|          50|              BA|           S|\n",
      "|     2023|              SE|             FT|       Data Engineer|130000|            USD|       130000|                US|           0|              US|           M|\n",
      "|     2023|              SE|             FT|       Data Engineer| 75000|            USD|        75000|                US|           0|              US|           M|\n",
      "|     2023|              SE|             FT|       Data Engineer|252000|            USD|       252000|                US|           0|              US|           M|\n",
      "|     2023|              SE|             FT|       Data Engineer|129000|            USD|       129000|                US|           0|              US|           M|\n",
      "|     2023|              MI|             FT|        Data Analyst|150000|            USD|       150000|                US|           0|              US|           M|\n",
      "+---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to-do \n",
    "salaries_df.write.format('csv').mode('overwrite').save('target/', header='true') #writing\n",
    "\n",
    "loaded_df = spark.read.csv('target/', inferSchema=True, header='true') #loading\n",
    "loaded_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88973ed",
   "metadata": {},
   "source": [
    "# **Introduction to SparkSQL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c51bdd",
   "metadata": {},
   "source": [
    "Spark SQL is a Spark module for structured data processing. It is used to query structured data inside Spark programs, using either SQL or a familiar DataFrame API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8e6099",
   "metadata": {},
   "source": [
    "####  Create a Table View\n",
    "Creating a table view in Spark SQL is required to run SQL queries programmatically on a DataFrame. A view is a temporary table to run SQL queries. A Temporary view provides local scope within the current Spark session. In this example we create a temporary view using the `createTempView()` function. Once we have a table view, we can run queries similar to querying a SQL table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "96ef4a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries_df.createTempView(\"salaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc34fe7",
   "metadata": {},
   "source": [
    "####  Running SQL queries and aggregating data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7c97f2",
   "metadata": {},
   "source": [
    "####  Task 8:\n",
    "Display the first five records of AI Engineers with salaries over 10,000 with Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "50e81fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+---------------+-----------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "|work_year|experience_level|employment_type|  job_title|salary|salary_currency|salary_in_usd|employee_residence|remote_ratio|company_location|company_size|\n",
      "+---------+----------------+---------------+-----------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "|     2024|              SE|             FT|AI Engineer|202730|            USD|       202730|                US|           0|              US|           M|\n",
      "|     2024|              SE|             FT|AI Engineer| 92118|            USD|        92118|                US|           0|              US|           M|\n",
      "|     2024|              MI|             FT|AI Engineer|150000|            USD|       150000|                US|         100|              US|           M|\n",
      "|     2024|              MI|             FT|AI Engineer| 90200|            USD|        90200|                US|         100|              US|           M|\n",
      "|     2024|              SE|             FT|AI Engineer|236872|            USD|       236872|                US|           0|              US|           M|\n",
      "+---------+----------------+---------------+-----------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to-do\n",
    "spark.sql(\"SELECT * \\\n",
    "          FROM salaries \\\n",
    "          WHERE job_title = 'AI Engineer' \\\n",
    "          AND salary > 10000;\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a498445",
   "metadata": {},
   "source": [
    "#### Task 9:\n",
    "Execute a SQL query to find and display the maximum salaries for job titles in `large` `US` companies, then plot the top 10 highest salaries using a horizontal bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7960ac8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|           job_title|max_salary_in_usd|\n",
      "+--------------------+-----------------+\n",
      "|Applied Machine L...|           423000|\n",
      "|      Data Scientist|           412000|\n",
      "| Data Analytics Lead|           405000|\n",
      "|  Research Scientist|           405000|\n",
      "|Applied Data Scie...|           380000|\n",
      "|Data Science Tech...|           375000|\n",
      "|   Applied Scientist|           350000|\n",
      "|Machine Learning ...|           342300|\n",
      "|Director of Data ...|           325000|\n",
      "|        Data Science|           324100|\n",
      "|Managing Director...|           300000|\n",
      "|        Head of Data|           290000|\n",
      "|        AI Architect|           289000|\n",
      "|         ML Engineer|           287700|\n",
      "|  Lead Data Engineer|           276000|\n",
      "|Data Science Manager|           272400|\n",
      "|   Research Engineer|           261500|\n",
      "|  AWS Data Architect|           258000|\n",
      "|Data Analytics Ma...|           254500|\n",
      "|Cloud Data Architect|           250000|\n",
      "+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to-do\n",
    "spark.sql(\"SELECT job_title, max(salary_in_usd) as max_salary_in_usd \\\n",
    "          FROM salaries \\\n",
    "          WHERE company_location = 'US' \\\n",
    "          AND company_size = 'L' \\\n",
    "          GROUP BY job_title \\\n",
    "          ORDER BY max_salary_in_usd DESC;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd4314b",
   "metadata": {},
   "source": [
    "#### Task 10:\n",
    "Write a Spark SQL query to count the instances where a Data Engineer's salary in USD is greater than that of a Data Analyst, considering matching experience level, employment type, and company location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ea4b02a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:==================================================>   (187 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 2060983|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# to do\n",
    "spark.sql(\"SELECT count(*) \\\n",
    "          FROM salaries S1 join salaries S2 \\\n",
    "          USING (experience_level, employment_type, company_location) \\\n",
    "          WHERE S1.job_title = 'Data Engineer' \\\n",
    "          AND S2.job_title = 'Data Analyst' \\\n",
    "          AND S1.salary_in_usd > S2.salary_in_usd;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c582f",
   "metadata": {},
   "source": [
    "#### Create an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "641c2391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to RDD\n",
    "rdd = salaries_df.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f4a533",
   "metadata": {},
   "source": [
    "#### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6833ab",
   "metadata": {},
   "source": [
    "A transformation is an operation on an RDD that results in a new RDD. The transformed RDD is generated rapidly because the new RDD is lazily evaluated, which means that the calculation is not carried out when the new RDD is generated. The RDD will contain a series of transformations, or computation instructions, that will only be carried out when an action is called. Note the use of the lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "de4eec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_salary_rdd = rdd.filter(lambda row: row['salary'] > 200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db75fce6",
   "metadata": {},
   "source": [
    "#### Actions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31bcb68",
   "metadata": {},
   "source": [
    "A transformation returns a result to the driver. We now apply the `count()` action to get the output from the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "23aef4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3262"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_salary_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c17eb62",
   "metadata": {},
   "source": [
    "#### Task 11:\n",
    "Create an RDD with integers from 1-50. Apply a transformation to multiply every number by 2, resulting in an RDD that contains the first 50 even numbers. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f3188e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98]\n"
     ]
    }
   ],
   "source": [
    "# to do\n",
    "rdd = sc.parallelize(range(1, 50))\n",
    "even_numbers = rdd.map(lambda number: number * 2)\n",
    "results = even_numbers.collect()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56f118f",
   "metadata": {},
   "source": [
    "#### Task 12:\n",
    "Filter and collect all records from an RDD for employees with the job title \"Data Science Lead\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3d963365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(work_year=2024, experience_level='SE', employment_type='FT', job_title='Data Science Lead', salary=185000, salary_currency='USD', salary_in_usd=185000, employee_residence='US', remote_ratio=0, company_location='US', company_size='M'),\n",
       " Row(work_year=2024, experience_level='SE', employment_type='FT', job_title='Data Science Lead', salary=120000, salary_currency='USD', salary_in_usd=120000, employee_residence='US', remote_ratio=0, company_location='US', company_size='M'),\n",
       " Row(work_year=2023, experience_level='SE', employment_type='FT', job_title='Data Science Lead', salary=148300, salary_currency='USD', salary_in_usd=148300, employee_residence='US', remote_ratio=0, company_location='US', company_size='M'),\n",
       " Row(work_year=2023, experience_level='SE', employment_type='FT', job_title='Data Science Lead', salary=84700, salary_currency='USD', salary_in_usd=84700, employee_residence='US', remote_ratio=0, company_location='US', company_size='M'),\n",
       " Row(work_year=2023, experience_level='SE', employment_type='FT', job_title='Data Science Lead', salary=272000, salary_currency='USD', salary_in_usd=272000, employee_residence='US', remote_ratio=0, company_location='US', company_size='M'),\n",
       " Row(work_year=2023, experience_level='SE', employment_type='FT', job_title='Data Science Lead', salary=170000, salary_currency='USD', salary_in_usd=170000, employee_residence='US', remote_ratio=0, company_location='US', company_size='M'),\n",
       " Row(work_year=2023, experience_level='SE', employment_type='FT', job_title='Data Science Lead', salary=204500, salary_currency='USD', salary_in_usd=204500, employee_residence='US', remote_ratio=0, company_location='US', company_size='M'),\n",
       " Row(work_year=2023, experience_level='SE', employment_type='FT', job_title='Data Science Lead', salary=142200, salary_currency='USD', salary_in_usd=142200, employee_residence='US', remote_ratio=0, company_location='US', company_size='M'),\n",
       " Row(work_year=2023, experience_level='MI', employment_type='FT', job_title='Data Science Lead', salary=185900, salary_currency='USD', salary_in_usd=185900, employee_residence='US', remote_ratio=0, company_location='US', company_size='M'),\n",
       " Row(work_year=2023, experience_level='MI', employment_type='FT', job_title='Data Science Lead', salary=129300, salary_currency='USD', salary_in_usd=129300, employee_residence='US', remote_ratio=0, company_location='US', company_size='M'),\n",
       " Row(work_year=2023, experience_level='MI', employment_type='FT', job_title='Data Science Lead', salary=185900, salary_currency='USD', salary_in_usd=185900, employee_residence='US', remote_ratio=0, company_location='US', company_size='M'),\n",
       " Row(work_year=2023, experience_level='MI', employment_type='FT', job_title='Data Science Lead', salary=129300, salary_currency='USD', salary_in_usd=129300, employee_residence='US', remote_ratio=0, company_location='US', company_size='M'),\n",
       " Row(work_year=2023, experience_level='MI', employment_type='FT', job_title='Data Science Lead', salary=225000, salary_currency='USD', salary_in_usd=225000, employee_residence='US', remote_ratio=0, company_location='US', company_size='M'),\n",
       " Row(work_year=2023, experience_level='MI', employment_type='FT', job_title='Data Science Lead', salary=152700, salary_currency='USD', salary_in_usd=152700, employee_residence='US', remote_ratio=0, company_location='US', company_size='M'),\n",
       " Row(work_year=2023, experience_level='MI', employment_type='FT', job_title='Data Science Lead', salary=191765, salary_currency='USD', salary_in_usd=191765, employee_residence='US', remote_ratio=0, company_location='US', company_size='M'),\n",
       " Row(work_year=2023, experience_level='MI', employment_type='FT', job_title='Data Science Lead', salary=134326, salary_currency='USD', salary_in_usd=134326, employee_residence='US', remote_ratio=0, company_location='US', company_size='M'),\n",
       " Row(work_year=2023, experience_level='MI', employment_type='FT', job_title='Data Science Lead', salary=244000, salary_currency='USD', salary_in_usd=244000, employee_residence='US', remote_ratio=0, company_location='US', company_size='M'),\n",
       " Row(work_year=2023, experience_level='MI', employment_type='FT', job_title='Data Science Lead', salary=200000, salary_currency='USD', salary_in_usd=200000, employee_residence='US', remote_ratio=0, company_location='US', company_size='M'),\n",
       " Row(work_year=2021, experience_level='MI', employment_type='FT', job_title='Data Science Lead', salary=150000, salary_currency='USD', salary_in_usd=150000, employee_residence='US', remote_ratio=100, company_location='US', company_size='M'),\n",
       " Row(work_year=2023, experience_level='SE', employment_type='FT', job_title='Data Science Lead', salary=247500, salary_currency='USD', salary_in_usd=247500, employee_residence='US', remote_ratio=0, company_location='US', company_size='M'),\n",
       " Row(work_year=2023, experience_level='SE', employment_type='FT', job_title='Data Science Lead', salary=172200, salary_currency='USD', salary_in_usd=172200, employee_residence='US', remote_ratio=0, company_location='US', company_size='M'),\n",
       " Row(work_year=2023, experience_level='MI', employment_type='FT', job_title='Data Science Lead', salary=60000, salary_currency='GBP', salary_in_usd=73824, employee_residence='GB', remote_ratio=0, company_location='GB', company_size='M'),\n",
       " Row(work_year=2023, experience_level='MI', employment_type='FT', job_title='Data Science Lead', salary=50000, salary_currency='GBP', salary_in_usd=61520, employee_residence='GB', remote_ratio=0, company_location='GB', company_size='M'),\n",
       " Row(work_year=2023, experience_level='SE', employment_type='FT', job_title='Data Science Lead', salary=225900, salary_currency='USD', salary_in_usd=225900, employee_residence='US', remote_ratio=0, company_location='US', company_size='M'),\n",
       " Row(work_year=2023, experience_level='SE', employment_type='FT', job_title='Data Science Lead', salary=156400, salary_currency='USD', salary_in_usd=156400, employee_residence='US', remote_ratio=0, company_location='US', company_size='M'),\n",
       " Row(work_year=2022, experience_level='SE', employment_type='FT', job_title='Data Science Lead', salary=165000, salary_currency='USD', salary_in_usd=165000, employee_residence='US', remote_ratio=50, company_location='US', company_size='S')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to do\n",
    "rdd = salaries_df.rdd\n",
    "filtered_rdd = rdd.filter(lambda row: row['job_title'] == 'Data Science Lead')\n",
    "filtered_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6201c7",
   "metadata": {},
   "source": [
    "#### Task 13:\n",
    "Calculate the total `salary` paid by `large` companies by filtering and aggregating salary data in an RDD using Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ba56dfd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320740143"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to do\n",
    "rdd = salaries_df.rdd\n",
    "filtered_rdd = rdd.filter(lambda row: row['company_size'] == 'L')\n",
    "sum_salaries = filtered_rdd.aggregate(0,\n",
    "                                      lambda acc, row: acc + row['salary'],\n",
    "                                      lambda acc1, acc2: acc1 + acc2)\n",
    "sum_salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6efa79",
   "metadata": {},
   "source": [
    "#### Task 14:\n",
    "Filter and display the top five highest-paying fully remote job titles based on salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ff758138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Title: Data Scientist, Salary: 30400000\n",
      "Job Title: Data Scientist, Salary: 6600000\n",
      "Job Title: Data Engineer, Salary: 4450000\n",
      "Job Title: Data Scientist, Salary: 4200000\n",
      "Job Title: Data Scientist, Salary: 4000000\n"
     ]
    }
   ],
   "source": [
    "# to do\n",
    "rdd = salaries_df.rdd\n",
    "remote_rdd = rdd.filter(lambda row: row['remote_ratio'] == 100)\n",
    "sorted_rdd = remote_rdd.sortBy(lambda row: row['salary'], ascending=False)\n",
    "for item in sorted_rdd.take(5):\n",
    "    print(f\"Job Title: {item['job_title']}, Salary: {item['salary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f79e6",
   "metadata": {},
   "source": [
    "##  Create a Pandas UDF\n",
    "Apache Spark has become the de-facto standard in processing big data. To enable data scientists to leverage the value of big data, Spark added a Python API in version 0.7, with support for user-defined functions (UDF). These user-defined functions operate one-row-at-a-time, and thus suffer from high serialization and invocation overhead. As a result, many data pipelines define UDFs in Java and Scala and then invoke them from Python.\n",
    "\n",
    "Pandas UDFs built on top of Apache Arrow bring you the ability to define low-overhead, high-performance UDFs entirely in Python.\n",
    "\n",
    "In addition, UDFs can be registered and invoked in SQL out of the box by registering a regular python function using the `@pandas_udf()` decorator.  \n",
    "\n",
    "[pyspark.sql.functions.udf](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html)\n",
    "\n",
    "[pyspark.sql.DataFrame.withColumn](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3671ed6",
   "metadata": {},
   "source": [
    "#### Task15:\n",
    "Implement a Pandas UDF in PySpark named `categorize_salary` that accepts a pandas Series containing salaries and categorizes each salary into \"Low\" (less than $50,000), \"Medium\" ($50,000 to $149,999), or \"High\" ($150,000 and above) by returning a new Series with these categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f8278e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do\n",
    "@pandas_udf(StringType())\n",
    "def categorize_salary(salary_series: pd.Series) -> pd.Series:\n",
    "    categories = []\n",
    "    for salary in salary_series:\n",
    "        if salary < 50000:\n",
    "            categories.append(\"Low\")\n",
    "        elif 50000 <= salary < 150000:\n",
    "            categories.append(\"Medium\")\n",
    "        else:\n",
    "            categories.append(\"High\")\n",
    "    return pd.Series(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46257f69",
   "metadata": {},
   "source": [
    "#### Task 16:\n",
    "Integrate the `categorize_salary` UDF into a PySpark DataFrame by adding a new column salary_category which classifies each salary. Subsequently, display the original salary and its corresponding category to verify the successful application of the UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2e5a1029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|salary|salary_category|\n",
      "+------+---------------+\n",
      "|202730|           High|\n",
      "| 92118|         Medium|\n",
      "|130500|         Medium|\n",
      "| 96000|         Medium|\n",
      "|190000|           High|\n",
      "|160000|           High|\n",
      "|400000|           High|\n",
      "| 65000|         Medium|\n",
      "|101520|         Medium|\n",
      "| 45864|            Low|\n",
      "|172469|           High|\n",
      "|114945|         Medium|\n",
      "|200000|           High|\n",
      "|150000|           High|\n",
      "|156450|           High|\n",
      "|119200|         Medium|\n",
      "|170000|           High|\n",
      "|130000|         Medium|\n",
      "|222200|           High|\n",
      "|136000|         Medium|\n",
      "+------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# to do\n",
    "classified_df = salaries_df.withColumn(\"salary_category\", categorize_salary(col(\"salary\")))\n",
    "classified_df.select(\"salary\", \"salary_category\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55781eae",
   "metadata": {},
   "source": [
    "#### Bonus 1:\n",
    "1.Calculating sum of salary with company size 'L' without caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c8b9b228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2176377773284912"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = salaries_df.rdd\n",
    "\n",
    "start_time_no_cache = time.time()\n",
    "dummy = rdd.filter(lambda row: row['company_size'] == 'L').aggregate(0,\n",
    "                                                                     lambda acc, row: acc + row['salary'],\n",
    "                                                                     lambda acc1, acc2: acc1 + acc2)\n",
    "end_time_no_cache = time.time()\n",
    "end_time_no_cache - start_time_no_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb7bb6",
   "metadata": {},
   "source": [
    "2.Calculating sum of salary with company size 'L' with caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0184958b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3096776008605957"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_cached = rdd.cache()\n",
    "\n",
    "start_time_cache = time.time()\n",
    "dummy = rdd_cached.filter(lambda row: row['company_size'] == 'L').aggregate(0,\n",
    "                                                                            lambda acc, row: acc + row['salary'],\n",
    "                                                                            lambda acc1, acc2: acc1 + acc2)\n",
    "end_time_cache = time.time()\n",
    "end_time_cache - start_time_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2521857f",
   "metadata": {},
   "source": [
    "#### Bonus 2:\n",
    "1.Narrow transforamtion example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "99bbc025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42276501655578613"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1, 1000000))\n",
    "\n",
    "start_time_narrow = time.time()\n",
    "narrow_result = rdd.map(lambda x: x * 2).collect()\n",
    "end_time_narrow = time.time()\n",
    "end_time_narrow - start_time_narrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226a929b",
   "metadata": {},
   "source": [
    "2.Wide transforamtion example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0be8846a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.953167200088501"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_map = rdd.map(lambda x: (x % 100, x))\n",
    "\n",
    "start_time_wide = time.time()\n",
    "wide_result = rdd_map.groupByKey().mapValues(list).collect()\n",
    "end_time_wide = time.time()\n",
    "end_time_wide - start_time_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "acc28789",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467dc573",
   "metadata": {},
   "source": [
    "## Authors\n",
    "[Mohammad Rahmanian](https://github.com/Mohammad-Rahmanian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f3a484",
   "metadata": {},
   "source": [
    "### Other Contributors\n",
    "[Mohammad Sadegh Mohammadi](https://github.com/sadegh-msm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
